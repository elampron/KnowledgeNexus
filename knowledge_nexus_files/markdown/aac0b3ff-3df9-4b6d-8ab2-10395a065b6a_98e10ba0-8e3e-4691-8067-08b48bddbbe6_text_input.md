Next Steps
Define the New Models in models/topics.py and models/memories.py (or a single file).
Write the DB create/merge logic in db/topics.py and db/memories.py.
Update the LLM extraction function so it calls GPT with instructions to separate Entities, Topics, and Memories.
Revamp or add a process_extracted_knowledge method in your pipeline to handle all the extracted items.
Refactor any existing code so that it deals gracefully with an expanded extraction result. Possibly rename from “entity” pipeline to a more general “knowledge_extraction” pipeline.
Add Vector Embeddings for memories or topics if you plan to do semantic search.
Test thoroughly with various sample texts to see if the LLM is effectively distinguishing Entities vs. Topics. Adjust your prompts or post-processing logic as needed (some items might straddle the line, so you can define fallback rules in code).
This approach ensures you can keep your system flexible: some text references might become Entity nodes, some might be Topic nodes, and some might be short “Memories” capturing key facts—each with its own place in the knowledge graph. Over time, you can refine how the LLM decides classification (e.g., if it sees “AI Agents” as a concept, that’s a Topic, but “Eric Lampron” is an Entity). Meanwhile, any direct statements or interesting facts become Memories that you can vector-search later.